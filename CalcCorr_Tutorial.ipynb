{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook serves as a tutorial to show how to use the CalcCorrections class to quickly and easily perform simulation-driven kinematic corrections for any particle, to correct for any systematic drift caused by the reconstuction of data, by comparing what was generated to what was reconstructed.\n",
    "\n",
    "Approach:\n",
    "   * Plot 2D histograms of the difference(reconstructed - generated) vs. reconstructed for which we want to correct.\n",
    "   * The 2D distribution is then \"sliced\" into bins in the x-axis (reconstructed), and projections in y-axis (difference). \n",
    "   * Each slice is then fitted to obtain the mean. \n",
    "   * These means are then plotted against the central position of the slice in a scatter graph, which is then fitted to produce a relationship which can be used to perform corrections.\n",
    "\n",
    "\n",
    "This 'package' has been designed to be largely automated, with the ability to interractively make adjustments on the results of the fits in order to refine them.  The use of Jupyter notebook serves as a shortcut to a GUI, where these adjustments can be made with some simple commands (all of these will be demonstrated below).\n",
    "\n",
    "Almost all of the required code is \"under the hood\" such that someone with little-to-no Python experience should be able to use this notebook, and with minimal tweaking, perform corrections of their own. The full code can be viewed in SimKinCorr.py and utils.py. It has been developed for readability, and has been extensively commented.\n",
    "\n",
    "Python does not have a native histogramming tool as powerful as something like ROOT.  Therefore, we use the [Boost_Histogram](https://github.com/scikit-hep/boost-histogram) package for ease of slicing, and projecting the 2D histograms. It is part of the [scikit-HEP project](https://scikit-hep.org/).\n",
    "\n",
    "   ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uproot\n",
    "import boost_histogram as bh\n",
    "import SimKinCorr as skc\n",
    "\n",
    "#interractive plots in JupyterLab\n",
    "%matplotlib widget \n",
    "\n",
    "#interractive plots in Jupyter-Notebook\n",
    "#%matplotlib widget\n",
    "# NB: ONLY LEAVE ONE UNCOMMENTED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "Data used for this example can be downloaded from: /work/clas12/pnaidoo/CalcCorr/tutorial_data.root\n",
    "\n",
    "In my case, I performed my analysis using [clas12root](https://github.com/JeffersonLab/clas12root), which means my processed data is in the form of a ROOT file. [Uproot](https://github.com/scikit-hep/uproot) (another packeage from the [scikit-HEP project](https://scikit-hep.org/)) is used to interface between the root file and [pandas-dataframes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) which are commonly used for working with and analysing the data in python.\n",
    "\n",
    "Pandas is a mature library, with interfaces to many data formats, so this can be adapted to suit your needs if you are confident enough.  Although I have never used it personally, [hipopy](https://github.com/JeffersonLab/hipo_tools#hipopy) is included in hipo-utils, and serves as an interface between the HiPO format and Python.\n",
    "\n",
    "*__WARNING__: Pandas dataframes hold data RAM. This has the benefit of being very fast, but if you try to open a file larger than the volume of RAM on your system, your system will likely crash.  \n",
    "As we are using simulated data it is assumed that it is unlikely that this will be an issue on any modern system.  However, there are ways around this, and if demand is sufficient I can implement a work around.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set destination of data file, and the name of the tree it is stored in.\n",
    "#-- tree in tutorial_data.root is called \"data\"\n",
    "data_file = \"/home/pauln/code/neutkincorr/sim_driven/output/ordered.root\" \n",
    "\n",
    "#data_file = \"/PATH/TO/tutorial_data.root\" \n",
    "data_tree_name = \"data\"\n",
    "\n",
    "#Open it with uproot, and convert into a Pandas Dataframe.\n",
    "data = uproot.open(data_file)[data_tree_name]\n",
    "df = data.pandas.df() \n",
    "\n",
    "# You can quickly view the variable-names/branches you have read in (eg. for easy copy/pasting) by using the command: list(df.columns)\n",
    "#list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "### Perform any calculations required:\n",
    "In my case, I did not perform the calculations of the difference in reconstructed and generated kinematics.  It's no problem, as this can be done (very quickly) with Pandas.\n",
    "Each line below appends a new column to our dataframe object, with the result of the calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate difference of reconstructed - generated and append to our dataframe\n",
    "df['diff_neut_px'] = df.rec_neut_px - df.gen_neut_px;\n",
    "df['diff_neut_py'] = df.rec_neut_py - df.gen_neut_py;\n",
    "df['diff_neut_pz'] = df.rec_neut_pz - df.gen_neut_pz;\n",
    "df['diff_neut_magP'] = df.rec_neut_magP - df.gen_neut_magP;\n",
    "df['diff_neut_pT'] = df.rec_neut_pT - df.gen_neut_pT;\n",
    "df['diff_neut_theta'] = df.rec_neut_theta - df.gen_neut_theta;\n",
    "df['diff_neut_phi'] = df.rec_neut_phi - df.gen_neut_phi;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " match are we talking about here? Are you talking about a regular expression match, or simply a if ... in my_raw_string### Prepare what CalcCorrections needs:\n",
    "I have tried to make what is required to run the corrections as minimal, and sensible as possible. Other than the data, it requires two things: the cuts you want to perform on the data, and the set up for the variable you wish to correct.\n",
    "\n",
    "#### Cuts\n",
    "Cuts should be provided as a list of bracketed statements as shown below.  \n",
    "Each of these statements return a boolean filter, of equal length as the number of events in our dataframe.  They are combined within the class to apply the cuts to the data when filling the 2D histogram.\n",
    "\n",
    "Cuts defined below are:\n",
    "   * $\\delta t \\le 0.4$ $GeV/c^{2}$\n",
    "   \n",
    "   * $\\delta\\phi_{copl.} \\le 5^{\\circ}$\n",
    "   \n",
    "   * Missing spectator-momentum:  \n",
    "     $MP_{eD->e'n'\\gamma X} \\le 0.7$ $ GeV/c^{2}$ \n",
    "   \n",
    "   * $\\theta^{cone}_{nX}\\le 20^{\\circ} $\n",
    "   \n",
    "   * Missing total transverse- momentum:  \n",
    "      $MP^{T}_{en->e'n'\\gamma X} \\le 0.12$ $ GeV/c^{2}$ \n",
    "   \n",
    "If you performed all of your cutting in your upstream processing, and only have exactly the events which you want to process in your data, you can simply ommit the cuts argument when initialising the class.\n",
    "\n",
    "#### Set-up Correction\n",
    "For this, we use a python dictionary - a special kind of container which contains labeled values. \n",
    "Below we set up $p_{x}$, and the fields of the dictionary are:\n",
    "   * `var`: The name of the variable in the dataframe we wish to correct.\n",
    "   * `v_bins_range`: A tuple which contains (# of bins, low range limit, high range limit) for `var`.\n",
    "   * `v_label`: A plot label for `var` which will be used in the figure.\n",
    "   * `delta`: The name of the difference of the target variable we are correcting.\n",
    "   * `d_bins_range`: A tuple which contains (# of bins, low range limit, high range limit) for `delta`.\n",
    "   * `d_label`: A plot label for `delta` which will be used in the figure.\n",
    "\n",
    "Every field in this setup object must be supplied, and everything on the left hand side of the colon should be left as is, as these labels are used in the inner working of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = [\n",
    "    (abs(df.rec_dt) <= 0.4),\n",
    "    (abs(df.rec_cop) <= 5),\n",
    "    (df.rec_MPspect <= 0.7),\n",
    "    (df.rec_neutcone <= 20),\n",
    "    (df.rec_MPT_tot <= 0.12)\n",
    "]\n",
    "\n",
    "\n",
    "px = {\n",
    "    'var': 'rec_neut_px',\n",
    "    'v_bins_range': (70, -0.5, 0.5),\n",
    "    'v_label': 'rec $p_{x}$',\n",
    "    'delta': 'diff_neut_px',\n",
    "    'd_bins_range': (70, -0.1, 0.1),\n",
    "    'd_label': '$\\delta p_{x}$ (recon - gen)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections\n",
    "### Running Corrections \n",
    "With our cuts and our setup defined, we are ready! \n",
    "Remember to assign the instance to a variable so that we can make adjustments to anything which needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_px = skc.CalcCorrections(data=df, setup=px, cuts=cuts)\n",
    "# you can also specify how many slices you would like by passing \"n_slices = N\" as an argument. Default is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting slices\n",
    "Once we have produced our instance of the class, it is time to visually inspect the fits and Chi-Squareds, and see if we need to make any changes.\n",
    "We have several options to manually tweak any slice which was either fitted poorly, or whose fit failed. \n",
    "\n",
    "Individual slices can:\n",
    "   * Have their initial fit parameters hard coded\n",
    "   * Have their fit range changed\n",
    "   * Be rebinned if a particular slice has slow stats\n",
    "   * If you want to revert any of the above changes, the slice can be \n",
    "       restored.\n",
    "   * If a slice cannot be 'made to cooperate' it can be vetoed, meaning it \n",
    "       will not be included in the final fit of the means.  \n",
    "       _NB: slices can also be 'un-vetoed'._\n",
    "\n",
    "In every case, the distribution of the means is replotted and refitted, with the result updating in the figure.\n",
    "\n",
    "See the cells below for an example of each adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebin slice 9 by a factor 2.\n",
    "corr_px.rebin_slice(slc_idx=9, factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit fit range of slice 3 to between -0.04 and 0.25.\n",
    "corr_px.set_slice_fitrange(slc_idx=3, low=-0.04, high=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore slice 3 to its original fit\n",
    "corr_px.restore_slice(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore slice 9 to its original binning\n",
    "corr_px.restore_slice(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard code the parameters for slice 9\n",
    "corr_px.set_slice_fitparams(slc_idx=9, amp=10, mean=-0.01, sigma=0.1, const=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veto slice 9 from consideration\n",
    "corr_px.veto_slice(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unveto slice 9 so it is included in the fit\n",
    "corr_px.unveto_slice(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model used to fit the final result\n",
    "# Default: linear.  Options: po2, pol3. pol4\n",
    "corr_px.change_fit_model(\"pol3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an arbitrary model to fit the final result\n",
    "def model(x, k3, c):\n",
    "    return k3*x**3 + c\n",
    "\n",
    "corr_px.user_def_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Corrections \n",
    "We want to validate our corrections.  We do this by applying them to our simulated data. It is incredibly quick and easy to do this using the power of Panda's dataframes, and the CalcCorrections class! \n",
    "\n",
    "The fitmodel used, and the parameter values of that model from our results are stored in the instance of our class, `corr_px`!  Therefore in one line we can append a new column to our dataframe with the result of the correction.\n",
    "\n",
    "We can then simply reprocess the data as we did before.  If the correction worked, we expect values to be more-centred on, and less spread around zero.\n",
    "All we need to do is define a set-up dictionary and make a new instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every value in rec_neut_px, subtract f(rec_neut_px), using the resulting fit-values.\n",
    "df['corr_neut_px'] = df.rec_neut_px - corr_px.fit_model(df.rec_neut_px, *corr_px.fit_vals)\n",
    "\n",
    "# recalculate the difference between reconstructed and generated post-correction.\n",
    "df['corr_diff_neut_px'] = df.corr_neut_px- df.gen_neut_px;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up dictionary\n",
    "corrected_px = {\n",
    "    'var': 'corr_neut_px',\n",
    "    'v_bins_range': (70, -0.5, 0.5),\n",
    "    'v_label': 'corr. rec $p_{x}$',\n",
    "    'delta': 'corr_diff_neut_px',\n",
    "    'd_bins_range': (70, -0.1, 0.1),\n",
    "    'd_label': 'corr. $\\delta p_{x}$ (recon - gen)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new instance of the class\n",
    "val_px = skc.CalcCorrections(data=df, setup=corrected_px, cuts=cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi0-ana",
   "language": "python",
   "name": "pi0-ana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
